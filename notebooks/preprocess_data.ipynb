{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from pytube import YouTube\n",
    "import collections\n",
    "import cv2\n",
    "\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import nltk  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### engine for pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options = python.BaseOptions(model_asset_path='../models/pose_landmarker.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "mp_pose = mp.solutions.pose\n",
    "detector = mp_pose.Pose()\n",
    "#detector = vision.PoseLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe.python.solutions import drawing_utils, pose\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    pose_landmarks_list = detection_result.pose_landmarks.landmark\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Create NormalizedLandmarkList to hold the detected landmarks\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z)\n",
    "        for landmark in pose_landmarks_list\n",
    "    ])\n",
    "\n",
    "    # Draw landmarks on the image\n",
    "    drawing_utils.draw_landmarks(\n",
    "        annotated_image,\n",
    "        pose_landmarks_proto,\n",
    "        pose.POSE_CONNECTIONS,  # Ensure POSE_CONNECTIONS matches the number of landmarks detected\n",
    "        drawing_utils.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "        drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "    )\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "\n",
    "def calculate_angle(p1, p2):\n",
    "    x_diff = p2[0] - p1[0]\n",
    "    y_diff = p2[1] - p1[1]\n",
    "    return np.degrees(np.arctan2(y_diff, x_diff))\n",
    "\n",
    "# Funkcja do obliczania kąta obrotu sylwetki\n",
    "def calculate_body_rotation_angle(landmarks):\n",
    "    if landmarks is None:\n",
    "      return None\n",
    "    # Wykryte punkty charakterystyczne dla ramion (np. 11 i 12 dla lewego i prawego ramienia)\n",
    "    left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n",
    "    right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x,\n",
    "                      landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y]\n",
    "\n",
    "    # Oblicz kąt nachylenia linii ramion\n",
    "    angle = calculate_angle(left_shoulder, right_shoulder)\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "<Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"24fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">\n"
     ]
    }
   ],
   "source": [
    "#xwyPjhRoeNc\n",
    "#nhoikoUEI8U\n",
    "video_id = \"nhoikoUEI8U\"\n",
    "subtitles = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "print(len(subtitles))\n",
    "\n",
    "yt = YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n",
    "stream = yt.streams.filter(res=\"720p\").first()\n",
    "print(yt.streams.filter(res=\"720p\").first())\n",
    "destination_path = \"../videos\" \n",
    "\n",
    "video_file = stream.download(output_path=destination_path)\n",
    "\n",
    "\n",
    "def cv2_to_mediapipe_image(cv2_image):\n",
    "    rgb_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "    image = mp.solutions.mediapipe.python.solution_base.Image(\n",
    "        width=rgb_image.shape[1],\n",
    "        height=rgb_image.shape[0],\n",
    "        rgb_data=np.frombuffer(rgb_image.tobytes(), dtype=np.uint8)\n",
    "    )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is optional tool to run main loop faster without processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-159.95314546650425\n",
      "-101.32241813607328\n",
      "-83.49624206538516\n",
      "-65.19486770546543\n",
      "-43.7041466005945\n",
      "15.326723721672215\n",
      "-43.194385308915635\n",
      "-68.08071794647772\n",
      "-34.75050769173432\n",
      "-31.925907001003896\n",
      "-59.12850380115535\n",
      "-62.05253149986459\n",
      "-74.5069941174697\n",
      "-52.89103637028931\n",
      "-71.05167634308162\n",
      "-70.79403335686183\n",
      "-74.32549610794187\n",
      "-78.49193856504841\n",
      "-89.18665388758376\n",
      "-86.74710803086082\n",
      "-86.00601382699583\n",
      "-89.81017025720249\n",
      "-81.09202078665241\n",
      "-75.51980141094994\n",
      "-63.30707023294511\n",
      "-65.72870326851661\n",
      "-71.03531189717611\n",
      "-70.55011614876639\n",
      "-40.757738355695395\n",
      "-95.01456357794628\n",
      "84.78841358204654\n",
      "81.80470627518856\n",
      "81.31143989494406\n",
      "82.82644881842903\n",
      "85.10119499850238\n",
      "87.60298166775024\n",
      "88.04968985502019\n",
      "88.56812447933748\n",
      "91.6374012113704\n",
      "80.27986225306535\n",
      "79.55774385714595\n",
      "-10.283285365093223\n",
      "-3.9429571772672762\n",
      "-17.237859760329453\n",
      "-20.15624499762196\n",
      "-10.395824541920852\n",
      "77.89170675488701\n",
      "71.41998144415254\n",
      "73.9717810516022\n",
      "74.54619423515305\n",
      "76.89707625425153\n",
      "77.97324516573943\n",
      "77.38479755767705\n",
      "90.86721444661343\n",
      "89.02790946698447\n",
      "87.84029211703808\n",
      "88.15556640022572\n",
      "-31.16657653039742\n",
      "62.41813744168178\n",
      "85.90949265071303\n",
      "-109.43482709966688\n",
      "-147.3976764506195\n",
      "94.05822982513841\n",
      "96.76399990169939\n",
      "95.48042735278106\n",
      "91.36882035762801\n",
      "88.9670382135685\n",
      "91.68702713805048\n",
      "92.09577984020214\n",
      "90.16077840540449\n",
      "89.54117175035894\n",
      "88.23529115949624\n",
      "89.08243726170046\n",
      "90.13057975153066\n",
      "89.89262112735945\n",
      "81.85102853534681\n",
      "95.75949144603838\n",
      "93.28422673886845\n",
      "95.2661255097697\n",
      "13.325214438925155\n",
      "124.16738568356594\n",
      "81.06621695485427\n",
      "-13.067377186636328\n",
      "-14.711828277912138\n",
      "-13.439804495758953\n",
      "85.62233414536941\n",
      "87.19423308300304\n",
      "-2.1216553444168897\n",
      "-5.63560789988974\n",
      "-75.98122294322032\n",
      "69.96926445544884\n",
      "76.4305682701041\n",
      "83.46536835317995\n",
      "88.90900622137335\n",
      "92.96484801305162\n",
      "93.34171002963933\n",
      "93.55653819205912\n",
      "we will approach the squat in two phases\n",
      "85.29338634592669\n",
      "85.82322840652779\n",
      "93.12100071870472\n",
      "-45.137745571671786\n",
      "-38.12780643550081\n",
      "-31.007295614903654\n",
      "-29.029738474993025\n",
      "-32.333043706889505\n",
      "-18.764159108618237\n",
      "-14.291041142982564\n",
      "-10.498652355182028\n",
      "153.04859706649884\n",
      "119.4139964079397\n",
      "16.59614014867955\n",
      "8.093875683884535\n",
      "1.761292842408507\n",
      "1.825331616779508\n",
      "0.01724982829607457\n",
      "0.8464704207953511\n",
      "1.6262474138418106\n",
      "-27.575559058582733\n",
      "-57.763051765531166\n",
      "-56.1233550100113\n",
      "-50.273932955572874\n",
      "-46.559334129580414\n",
      "-47.56819280998601\n",
      "-45.3651283263748\n",
      "-41.80664849387919\n",
      "-18.815746098520904\n",
      "-8.991183682614933\n",
      "-4.69376973200849\n",
      "1.031259325120757\n",
      "3.666327068294512\n",
      "5.553962914863285\n",
      "7.202683442535865\n",
      "8.422238109467436\n",
      "7.7453108987604\n",
      "7.411651023175589\n",
      "7.75725421098175\n",
      "8.158068165795312\n",
      "8.363555652584006\n",
      "1.0687493913471418\n",
      "-9.188702757457454\n",
      "-22.085568811281306\n",
      "-15.028166518135391\n",
      "-19.164519466123764\n",
      "-25.11642047815434\n",
      "-31.664083982453096\n",
      "-35.899966678326116\n",
      "first unloaded to solve problems\n",
      "-32.29497829963338\n",
      "-28.084420595941094\n",
      "-20.750670099565856\n",
      "-17.922658043691836\n",
      "-38.84500076974555\n",
      "-35.0289033019539\n",
      "-20.288392375614965\n",
      "-23.172838009195647\n",
      "-26.18185482704618\n",
      "-35.365848592523335\n",
      "-39.67242574437994\n",
      "-39.706572266705855\n",
      "-37.02758447773128\n",
      "-38.30788325939637\n",
      "-44.146355754292934\n",
      "-38.058223901502686\n",
      "-46.876066636347154\n",
      "-41.377840852684244\n",
      "-29.205006932389804\n",
      "-34.3974892951349\n",
      "-35.4700018800548\n",
      "-33.70745160348492\n",
      "-21.579794810702683\n",
      "-24.144241133075056\n",
      "-39.03559477783897\n",
      "-40.90885508635399\n",
      "-43.203050928028446\n",
      "-47.81862722559158\n",
      "-55.894829323780904\n",
      "-17.278051265605523\n",
      "-31.382057543179357\n",
      "-39.645337358095205\n",
      "-46.777428021134845\n",
      "-39.56519727626872\n",
      "-122.59970504150078\n",
      "-167.41846094478976\n",
      "-133.66433676976519\n",
      "-159.02576926754114\n",
      "-166.71299373121573\n",
      "-173.80249426392783\n",
      "-174.57466688099353\n",
      "-167.8576177735002\n",
      "-6.755064842448383\n",
      "-28.856388650100037\n",
      "-40.10397890539665\n",
      "-45.313829285819516\n",
      "-69.54365736221769\n",
      "-87.94712353884582\n",
      "-34.97032678807386\n",
      "-45.76925831911048\n",
      "-33.2970995628628\n",
      "-146.00033924671558\n",
      "-80.20057417585969\n",
      "-48.138394187220534\n",
      "-43.23880840378866\n",
      "-30.327993204581016\n",
      "-24.831750358953407\n",
      "-22.37709791159849\n",
      "-21.30255122561347\n",
      "-21.552783862579698\n",
      "-15.191574687437944\n",
      "-13.371667000914643\n",
      "-13.88800278343854\n",
      "-14.253154350325588\n",
      "-17.40975948499171\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_file)\n",
    "dq = collections.deque()\n",
    "cv2.namedWindow('Video with Subtitles', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Video with Subtitles', 800, 600)\n",
    "last_frame = 0 \n",
    "current_frame = 0 \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "curr_sub_start = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "        current_time = current_frame / fps\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        if last_frame != current_frame:\n",
    "            while subtitles[curr_sub_start]['start'] < current_time:\n",
    "                print(subtitles[curr_sub_start]['text'])\n",
    "                dq.append(curr_sub_start)\n",
    "                curr_sub_start += 1\n",
    "            if len(dq) > 0:\n",
    "                while subtitles[dq[0]]['start'] + subtitles[dq[0]]['duration'] < current_time:\n",
    "                    dq.popleft()\n",
    "\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            edges = cv2.Canny(gray_frame, threshold1=100, threshold2=200)  \n",
    "\n",
    "            sub_index = 0\n",
    "            for x in dq:\n",
    "                cv2.putText(frame, subtitles[x]['text'], (50, 50 + 50 * sub_index), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                sub_index += 1\n",
    "\n",
    "            #img = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            detection_result = detector.process(frame)\n",
    "            if(detection_result.pose_landmarks):\n",
    "                #print(detection_result.pose_landmarks)\n",
    "                #print(detection_result.pose_landmarks.landmark[0])\n",
    "                \n",
    "                #print(detection_result.pose_landmarks[11])\n",
    "                body_angle = calculate_body_rotation_angle(detection_result.pose_landmarks.landmark)\n",
    "                print(body_angle)\n",
    "                #cv2.putText(frame, body_angle, (50, 500), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                annotated_image = draw_landmarks_on_image(frame, detection_result)\n",
    "                bgr_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imshow('Video with Subtitles', bgr_image)\n",
    "            else:\n",
    "               cv2.imshow('Video with Subtitles', frame) \n",
    "\n",
    "        #cv2.imshow('Video with Subtitles', frame)\n",
    "\n",
    "        #cv2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))    # Wait for user input (right arrow key to go to the next frame)\n",
    "        key = cv2.waitKey(30)  # Adjust the delay as needed (milliseconds)\n",
    "        last_frame = current_frame\n",
    "        if key == 27:  # ESC key to exit\n",
    "            break\n",
    "        elif key == 83 or key == 100:\n",
    "            current_frame += 1\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide text into sentences and add punctuation with ml model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4705\n",
      "we will approach the squat in two phasesfirst unloaded to solve problemsassociated with the bottom position andthen loaded to learn how to apply thebottom position to the hip drive usedfor heavier weights since the majorityof the problems with the squat happenedat the bottom this method expedites theprocess quite effectively we will use afairly neutral foot placement with theheels about shoulder width apart and thetoes pointed out at about 30 degreesmany people will assume a stance withtoes pointed too forward so you may needto point them out more than you want tonext you're going to assume the positionyou will be in at the bottom of a squatwithout the barsquat down all the way to a position inwhich the apex of the hip crease dropsjust below the top of the patella putyour elbows against your knees with thepalms of your hands together and shoveyour knees out notice your feet are flaton the floor your knees are shoved outto where they are in a parallel linewith your feet and just a little infront of your toes your back should beas flat as you can get it also noticethat your back is inclined at about a 45degree angle not at all vertical andyour eyes are looking down at the floora few feet in front of you after you'veestablished the bottom position come upout of the bottom by driving your buttstraight up in the air up not forwardnot back this movement keeps your weightsolidly over the whole foot instead ofletting it shift to the toes think abouta chain hook to your hips pulling youstraight up out of the bottom set therack height so that the bar is at aboutthe level of your mid sternum take aneven grip on the bar measured from themarkings placed on the bar for thispurpose a standard power bar has 16 to17 inches between the ends of the insideneural and 32 inches between the fingermarks grip width for the squat will varywith shoulder width and flexibility butin general the hands will be betweenthese two markings with the narrowestgrip you can manage a narrower gripallows a flexible person to bettersupport the bar with the posteriormuscles of the shoulders and a widergrip allows an inflexible person to getmore comfortable under the bar thethumbs should be placed on top of thebar so that the wrists can be held in astraight line with the forearms theelbows should be lifted up to trap thebar between the hands and the backelbows should be up but not high withyour grip in place and your hands andthumbs on top of the bar dip your headunder the bar and come up into positionwith the bar on your back just below thespine of the scapula the bone you feelat the top of the shoulder blades andthen secure it in place by lifting yourelbows and chest at the same time itshould feel as though the bar is restingon a shelf under the traps and on top ofthe posterior deltoids take the bar outof the rack in the same position inwhich it is to be squatted with thetorso and shoulders tight the chest andelbows up the head position down andboth feet under the bar step back justenough to clear the rack and assume thesame stance you used earlier again heelsshould be about shoulder width apartwith toes pointed out about 30 degreesat this point you are ready to squatwith the empty bar everything you'reabout to do is the same as you didunweighted only two things are differentone you don't have your elbows availableto help push your knees out so you needto do this with your brain and two don'tstop at the bottom just go down andimmediately come back up driving yourbutt straight up not forward not backout of the bottom now look down at aspot on the floor about four to fivefeet in front of you take a big breathand hold it and squat you should be ingood balance at the bottom of the squatwith your weight balanced evenly overyour feetneither on your heels nor forward onyour toesbalance problems usually indicate a backangle that is too verticalremember that the back angle will not bevertical at allsit back lean forward shove your kneesout point your nipples at the floorallow your hips to perform the squat notyour legs do not accept anything lessthan full depth ever if you are high itis usually because your knees are notout most people who have problems withthe squat do not shove their knees outenough do a set of five and rack the barwalk forward until the bar touches thevertical parts of the rack find theuprights not the hooks you can't missthe uprights and if you touch themyou'll be over the hooks the generalplan is to do a couple more sets of 5reps with the empty bar to nail down themovement pattern and then add weight doanother set of 5 and keep increasing ineven increments until the next increasewould compromise your form and that isthe first squat workout[Music]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Damian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4790\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text =''\n",
    "for obj in subtitles:\n",
    "    text+=obj['text']\n",
    "\n",
    "print(len(text))\n",
    "print(text)\n",
    "\n",
    "\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "model = PunctuationModel()\n",
    "\n",
    "result = model.restore_punctuation(text)\n",
    "print(len(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing artificial connections in words (auto generating subtitles from yt isn't ideal)\n",
    "### also pos-tags are added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(result)\n",
    "\n",
    "import wordsegment\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# segment powoduje również tokenizacje zdania dlatego ten etap(tokenizacji) zostanie pominięty.\n",
    "sents = [(segment(sent)) for sent in sents]\n",
    "#porter = nltk.PorterStemmer()\n",
    "#sents = [[porter.stem(t) for t in sent] for sent in sents]\n",
    "sents = [nltk.pos_tag(sent) for sent in sents]\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>} \n",
    "      {<NNP>+}               \n",
    "\"\"\"\n",
    "# grammar = r\"\"\"\n",
    "#   NP: {<DT>?<JJ>*<NN>}\n",
    "#   VP: {<VB.*><NP|PP>*}\n",
    "#   PP: {<IN><NP>}\n",
    "#   ADJP: {<JJ>}\n",
    "#   ADVP: {<RB.*>}\n",
    "# \"\"\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "\t\n",
    "# class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
    "\n",
    "#     def __init__(self, train_sents):\n",
    "#         train_set = []\n",
    "#         for tagged_sent in train_sents:\n",
    "#             untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "#             history = []\n",
    "#             for i, (word, tag) in enumerate(tagged_sent):\n",
    "#                 featureset = npchunk_features(untagged_sent, i, history) \n",
    "#                 train_set.append( (featureset, tag) )\n",
    "#                 history.append(tag)\n",
    "#         self.classifier = nltk.MaxentClassifier.train( \n",
    "#             train_set, algorithm='megam', trace=0)\n",
    "\n",
    "#     def tag(self, sentence):\n",
    "#         history = []\n",
    "#         for i, word in enumerate(sentence):\n",
    "#             featureset = npchunk_features(sentence, i, history)\n",
    "#             tag = self.classifier.classify(featureset)\n",
    "#             history.append(tag)\n",
    "#         return zip(sentence, history)\n",
    "\n",
    "# class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "#     def __init__(self, train_sents):\n",
    "#         tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "#                          nltk.chunk.tree2conlltags(sent)]\n",
    "#                         for sent in train_sents]\n",
    "#         self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "#     def parse(self, sentence):\n",
    "#         tagged_sents = self.tagger.tag(sentence)\n",
    "#         conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "#         return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "# def npchunk_features(sentence, i, history):\n",
    "#      word, pos = sentence[i]\n",
    "#      return {\"pos\": pos}\n",
    "# chunker = ConsecutiveNPChunker(train_sents)\n",
    "# print(chunker.evaluate(test_sents))\n",
    "\n",
    "\n",
    "# sents = [cp.parse(sent) for sent in sents]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('measured', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('markings', 'NNS'), ('placed', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('bar', 'NN'), ('for', 'IN'), ('this', 'DT'), ('purpose', 'NN'), ('a', 'DT'), ('standard', 'JJ'), ('powerbar', 'NN'), ('has', 'VBZ'), ('16to17', 'CD'), ('inches', 'NNS'), ('between', 'IN'), ('the', 'DT'), ('ends', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('inside', 'JJ'), ('neural', 'JJ'), ('and', 'CC'), ('32', 'CD'), ('inches', 'NNS'), ('between', 'IN'), ('the', 'DT'), ('finger', 'NN'), ('marks', 'NNS')]\n",
      "(S\n",
      "  again/RB\n",
      "  heels/NNS\n",
      "  should/MD\n",
      "  be/VB\n",
      "  about/IN\n",
      "  shoulder/NN\n",
      "  width/NNS\n",
      "  apart/RB\n",
      "  with/IN\n",
      "  toes/NNS\n",
      "  pointed/VBN\n",
      "  out/RP\n",
      "  about/IN\n",
      "  30/CD\n",
      "  degrees/NNS\n",
      "  at/IN\n",
      "  this/DT\n",
      "  point/NN\n",
      "  you/PRP\n",
      "  are/VBP\n",
      "  ready/JJ\n",
      "  to/TO\n",
      "  squat/VB\n",
      "  with/IN\n",
      "  the/DT\n",
      "  empty/JJ\n",
      "  bar/NN)\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('treebank')\n",
    "# print(sents[30])\n",
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "#print(sent)\n",
    "print(sents[15])\n",
    "#print(nltk.ne_chunk(sent))\n",
    "print(nltk.ne_chunk(sents[25]))\n",
    "\n",
    "# sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "# (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "# grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "\n",
    "# cp = nltk.RegexpParser(grammar) \n",
    "# result = cp.parse(sentence) \n",
    "# print(result) \n",
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "# print(cp.evaluate(sents))\n",
    "# result.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding sentences with technique rules(unfinished) (regexp: noun(body part) and verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will use a fairly neutral foot placement with the heels about shoulder width apart and the toes pointed out at about 30 degrees many people will assume a stance with toes pointed too forward so you may need to point them out more than you want\n",
      "your back should be as flat as you can get it\n",
      "also notice that your back is inclined at about a45 degree angle not at all vertical and your eyes are looking down at the floor a few feet in front of you\n",
      "this movement keeps your weight solidly over the whole foot instead of letting it shift to the toes\n",
      "grip width for the squat will vary with shoulder width and flexibility but in general the hands will be between these two markings\n",
      "the elbows should be lifted up to trap the bar between the hands and the back elbows should be up but not high\n",
      "with your grip in place and your hands and thumbs on top of the bar dip your head under the bar and come up into position with the bar on your back just below the spine of the scapula the bone you feel at the top of the shoulder blades and then secure it in place by lifting your elbows and chest at the same time\n",
      "take the bar out of the rack in the same position in which it is to be squatted with the torso and shoulders tight the chest and elbows up the head position down and both feet under the bar\n",
      "step back just enough to clear the rack and assume the same stance you used earlier\n",
      "again heels should be about shoulder width apart with toes pointed out about 30 degrees at this point you are ready to squat with the empty bar\n",
      "and two dont stop at the bottom just go down and immediately come back up driving your butt straight up not forward not back out of the bottom\n",
      "you should be in good balance at the bottom of the squat with your weight balanced evenly over your feet neither on your heels nor forward on your toes balance problems usually indicate a back angle that is too vertical remember that the back angle will not be vertical at all sit back lean forward shove your knees out point your nipples at the floor allow your hips to perform the squat not your legs\n"
     ]
    }
   ],
   "source": [
    "text = \"When performing squats with a barbell, ensure your back is straight, knees do not extend beyond your toes, and the barbell rests securely on your shoulders.\"\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "part = wn.synsets('body_part')[0]\n",
    "\n",
    "def is_body_part(candidate):\n",
    "    for ss in wn.synsets(candidate):\n",
    "        # only get those where the synset matches exactly\n",
    "        name = ss.name().split(\".\", 1)[0]\n",
    "        if name != candidate:\n",
    "            continue\n",
    "        hit = part.lowest_common_hypernyms(ss)\n",
    "        if hit and hit[0] == part:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# for word in sents[0]:\n",
    "#     print(is_body_part(word[0]), word[0], sep=\"\\t\")\n",
    "\n",
    "# Procesowanie każdego zdania\n",
    "# for sentence in sents:\n",
    "#     if any(is_body_part(t[0].lower()) for t in sentence):\n",
    "#         print(f\"Zdanie zawiera część ciała: {sentence}\")\n",
    "\n",
    "import nltk\n",
    "from nltk import CFG\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.parse import ChartParser\n",
    "\n",
    "# Lista części ciała\n",
    "body_parts = [\"head\", \"arm\", \"leg\", \"hand\", \"foot\", \"eye\", \"ear\", \"nose\", \"mouth\", \"shoulder\", \"knee\", \"elbow\"]\n",
    "\n",
    "# Definicja gramatyki bezkontekstowej z użyciem POS tags\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> DT JJNN | JJNN\n",
    "  VP -> VBZ NP | VBZ ADJP | VBZ PP\n",
    "  DT -> 'the' | 'a' | 'his' | 'her'\n",
    "  JJNN -> JJ NN | JJNN JJ NN\n",
    "  JJ -> 'badly' | 'quickly' | 'slowly' | 'fast'\n",
    "  NN -> 'head' | 'arm' | 'leg' | 'hand' | 'foot' | 'eye' | 'ear' | 'nose' | 'mouth' | 'shoulder' | 'knee' | 'elbow'\n",
    "  VBZ -> VB\n",
    "  ADJP -> JJ NP\n",
    "  NP -> DT JJNN\n",
    "  PP -> IN NP\n",
    "  IN -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "# Tworzenie parsera\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "# Tokenizacja tekstu na zdania\n",
    "for sentence in sents:\n",
    "    # Sprawdzanie czy zdanie pasuje do gramatyki\n",
    "    #print(sentence)\n",
    "    words = [word for word, tag in sentence]\n",
    "    try:\n",
    "        for tree in parser.parse(words):\n",
    "            # # Sprawdzanie czy pierwsza fraza rzeczownikowa jest częścią ciała\n",
    "            # np = tree[0]\n",
    "            # if np.label() == 'NP' and np[0][0].lower() in body_parts:\n",
    "            #     print(f\"Zdanie zawiera część ciała jako podmiot: {sentence}\")\n",
    "            tree.pretty_print()\n",
    "    except ValueError:\n",
    "            # Jeżeli parser nie znajdzie pasującego drzewa, przechodzi do następnego zdania\n",
    "        #print(\"nie ma drzewa\")    \n",
    "        continue\n",
    "import re \n",
    "\n",
    "#pattern = r'(head|arm|leg|hand|foot|eye|ear|nose|back|mouth|shoulder|knee|elbow)'\n",
    "pattern = r'\\b(head|arm|leg|hand|foot|eye|ear|nose|back|mouth|shoulder|knee|elbow)\\b\\s+(\\w+)'\n",
    "\n",
    "sents_for_regexp = nltk.sent_tokenize(result)\n",
    "sents_for_regexp = [(segment(sent)) for sent in sents_for_regexp]\n",
    "for sentence in sents_for_regexp:\n",
    "    #print(sentence)\n",
    "    sem = ' '.join([str(elem) for elem in sentence])\n",
    "    #print(sem)\n",
    "    match = re.search(pattern,sem)\n",
    "    if match:\n",
    "        print(sem)\n",
    "        #print(match.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will approach the squat in two phases first unloaded to solve problems associated with the bottom position and then loaded to learn how to apply the bottom position to the hip drive used for heavier weights \n",
      "\n",
      "since the majority of the problems with the squat happened at the bottom this method expedites the process quite effectively \n",
      "\n",
      "we will use a fairly neutral foot placement with the heels about shoulder width apart and the toes pointed out at about 30 degrees many people will assume a stance with toes pointed too forward so you may need to point them out more than you want \n",
      "\n",
      "to next your e going to assume the position you will be in at the bottom of a squat without the bar squat down all the way to a position in which the apex of the hip crease drops just below the top of the patella \n",
      "\n",
      "put your elbows against your knees with the palms of your hands together and shove your knees out \n",
      "\n",
      "notice your feet are flat on the floor \n",
      "\n",
      "your knees are shoved out to where they are in a parallel line with your feet and just a little in front of your toes \n",
      "\n",
      "your back should be as flat as you can get it \n",
      "\n",
      "also notice that your back is inclined at about a45 degree angle not at all vertical and your eyes are looking down at the floor a few feet in front of you \n",
      "\n",
      "after youve established the bottom position come up out of the bottom by driving your butt straight up in the air \n",
      "\n",
      "up not forward not back \n",
      "\n",
      "this movement keeps your weight solidly over the whole foot instead of letting it shift to the toes \n",
      "\n",
      "think about a chain hook to your hips pulling you straight up out of the bottom \n",
      "\n",
      "set the rack height so that the bar is at about the level of your mid sternum \n",
      "\n",
      "take an even grip on the bar \n",
      "\n",
      "measured from the markings placed on the bar for this purpose a standard powerbar has 16to17 inches between the ends of the inside neural and 32 inches between the finger marks \n",
      "\n",
      "grip width for the squat will vary with shoulder width and flexibility but in general the hands will be between these two markings \n",
      "\n",
      "with the narrowest grip you can manage \n",
      "\n",
      "a narrower grip allows a flexible person to better support the bar with the posterior muscles of the shoulders and a wider grip allows an inflexible person to get more comfortable under the bar \n",
      "\n",
      "the thumbs should be placed on top of the bar so that the wrists can be held in a straight line with the forearms \n",
      "\n",
      "the elbows should be lifted up to trap the bar between the hands and the back elbows should be up but not high \n",
      "\n",
      "with your grip in place and your hands and thumbs on top of the bar dip your head under the bar and come up into position with the bar on your back just below the spine of the scapula the bone you feel at the top of the shoulder blades and then secure it in place by lifting your elbows and chest at the same time \n",
      "\n",
      "it should feel as though the bar is resting on a shelf under the traps and on top of the posterior deltoids \n",
      "\n",
      "take the bar out of the rack in the same position in which it is to be squatted with the torso and shoulders tight the chest and elbows up the head position down and both feet under the bar \n",
      "\n",
      "step back just enough to clear the rack and assume the same stance you used earlier \n",
      "\n",
      "again heels should be about shoulder width apart with toes pointed out about 30 degrees at this point you are ready to squat with the empty bar \n",
      "\n",
      "everything you re about to do is the same as you did unweighted \n",
      "\n",
      "only two things are different one you dont have your elbows available to help push your knees out so you need to do this with your brain \n",
      "\n",
      "and two dont stop at the bottom just go down and immediately come back up driving your butt straight up not forward not back out of the bottom \n",
      "\n",
      "now look down at a spot on the floor about four to five feet in front of you take a big breath and hold it and squat \n",
      "\n",
      "you should be in good balance at the bottom of the squat with your weight balanced evenly over your feet neither on your heels nor forward on your toes balance problems usually indicate a back angle that is too vertical remember that the back angle will not be vertical at all sit back lean forward shove your knees out point your nipples at the floor allow your hips to perform the squat not your legs \n",
      "\n",
      "do not accept anything less than full depth ever \n",
      "\n",
      "if you are high it is usually because your knees are not out \n",
      "\n",
      "most people who have problems with the squat do not shove their knees out enough \n",
      "\n",
      "do a set of five and rack the bar walk forward until the bar touches the vertical parts of the rack \n",
      "\n",
      "find the uprights not the hooks \n",
      "\n",
      "you cant miss the uprights and if you touch them youll be over the hooks \n",
      "\n",
      "the general plan is to do a couple more sets of 5reps with the empty bar to nail down the movement pattern and then add weight do another set of 5 and keep increasing in even increments until the next increase would compromise your form \n",
      "\n",
      "and that is the first squat workout music \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    for tuple in sent:\n",
    "        print(tuple[0],end = \" \")\n",
    "    print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main loop of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a7b3005332d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mdetection_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mbody_angle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_body_rotation_angle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetection_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody_angle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6f60d52f5485>\u001b[0m in \u001b[0;36mcalculate_body_rotation_angle\u001b[1;34m(landmarks)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_body_rotation_angle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Wykryte punkty charakterystyczne dla ramion (np. 11 i 12 dla lewego i prawego ramienia)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     left_shoulder = [landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].x,\n\u001b[0m\u001b[0;32m     30\u001b[0m                      landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n\u001b[0;32m     31\u001b[0m     right_shoulder = [landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].x,\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_file)\n",
    "dq = collections.deque()\n",
    "cv2.namedWindow('Video with Subtitles', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Video with Subtitles', 800, 600)\n",
    "\n",
    "current_frame = 0 \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "curr_sub_start = 0\n",
    "while True:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "    current_time = current_frame / fps\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    while(subtitles[curr_sub_start]['start']<current_time):\n",
    "        print(subtitles[curr_sub_start]['text'])\n",
    "        dq.append(curr_sub_start)\n",
    "        curr_sub_start=curr_sub_start+1\n",
    "    if(len(dq) >0):\n",
    "        while(subtitles[dq[0]]['start'] + subtitles[dq[0]]['duration']<current_time):\n",
    "            dq.popleft()\n",
    "    \n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    edges = cv2.Canny(gray_frame, threshold1=100, threshold2=200)  \n",
    "\n",
    "    sub_index=0\n",
    "    for x in dq:\n",
    "        cv2.putText(frame, subtitles[x]['text'], (50, 50+50*sub_index), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        sub_index+=1\n",
    "\n",
    "\n",
    "    img = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "    detection_result = detector.detect(img)\n",
    "    # body_angle = calculate_body_rotation_angle(detection_result.pose_landmarks.landmark)\n",
    "    # cv2.putText(frame, body_angle, (50, 500), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    annotated_image = draw_landmarks_on_image(img.numpy_view(), detection_result)\n",
    "    bgr_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Display the image using OpenCV\n",
    "    #cv2.imshow('Video with Subtitles', frame)\n",
    "    cv2.imshow('Video with Subtitles', bgr_image)\n",
    "    #cv2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))    # Wait for user input (right arrow key to go to the next frame)\n",
    "    key = cv2.waitKey(30)  # Adjust the delay as needed (milliseconds)\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == 83 or key == 100:\n",
    "        current_frame += 1\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
